{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-10-30T15:23:53.086140Z","iopub.status.busy":"2022-10-30T15:23:53.085236Z","iopub.status.idle":"2022-10-30T15:23:53.123213Z","shell.execute_reply":"2022-10-30T15:23:53.122177Z","shell.execute_reply.started":"2022-10-30T15:23:53.086068Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/deberta-v3-large/deberta-v3-large/spm.model\n","/kaggle/input/deberta-v3-large/deberta-v3-large/config.json\n","/kaggle/input/deberta-v3-large/deberta-v3-large/README.md\n","/kaggle/input/deberta-v3-large/deberta-v3-large/tf_model.h5\n","/kaggle/input/deberta-v3-large/deberta-v3-large/tokenizer_config.json\n","/kaggle/input/deberta-v3-large/deberta-v3-large/pytorch_model.bin\n","/kaggle/input/feedback-prize-english-language-learning/sample_submission.csv\n","/kaggle/input/feedback-prize-english-language-learning/train.csv\n","/kaggle/input/feedback-prize-english-language-learning/test.csv\n","/kaggle/input/iterativestratification/.travis.yml\n","/kaggle/input/iterativestratification/setup.cfg\n","/kaggle/input/iterativestratification/LICENSE\n","/kaggle/input/iterativestratification/.gitignore\n","/kaggle/input/iterativestratification/README.md\n","/kaggle/input/iterativestratification/setup.py\n","/kaggle/input/iterativestratification/tests/test_ml_stratifiers.py\n","/kaggle/input/iterativestratification/tests/__init__.py\n","/kaggle/input/iterativestratification/iterstrat/ml_stratifiers.py\n","/kaggle/input/iterativestratification/iterstrat/__init__.py\n","/kaggle/input/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli/config.json\n","/kaggle/input/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli/merges.txt\n","/kaggle/input/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli/README.md\n","/kaggle/input/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli/vocab.json\n","/kaggle/input/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli/tokenizer_config.json\n","/kaggle/input/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli/bpe_encoder.bin\n","/kaggle/input/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli/pytorch_model.bin\n","/kaggle/input/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge/config.json\n","/kaggle/input/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge/merges.txt\n","/kaggle/input/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge/vocab.json\n","/kaggle/input/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge/tokenizer_config.json\n","/kaggle/input/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge/bpe_encoder.bin\n","/kaggle/input/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge/pytorch_model.bin\n","/kaggle/input/huggingface-deberta-variants/deberta-base-mnli/deberta-base-mnli/config.json\n","/kaggle/input/huggingface-deberta-variants/deberta-base-mnli/deberta-base-mnli/merges.txt\n","/kaggle/input/huggingface-deberta-variants/deberta-base-mnli/deberta-base-mnli/vocab.json\n","/kaggle/input/huggingface-deberta-variants/deberta-base-mnli/deberta-base-mnli/tokenizer_config.json\n","/kaggle/input/huggingface-deberta-variants/deberta-base-mnli/deberta-base-mnli/bpe_encoder.bin\n","/kaggle/input/huggingface-deberta-variants/deberta-base-mnli/deberta-base-mnli/pytorch_model.bin\n","/kaggle/input/huggingface-deberta-variants/deberta-large/deberta-large/config.json\n","/kaggle/input/huggingface-deberta-variants/deberta-large/deberta-large/merges.txt\n","/kaggle/input/huggingface-deberta-variants/deberta-large/deberta-large/vocab.json\n","/kaggle/input/huggingface-deberta-variants/deberta-large/deberta-large/tokenizer_config.json\n","/kaggle/input/huggingface-deberta-variants/deberta-large/deberta-large/bpe_encoder.bin\n","/kaggle/input/huggingface-deberta-variants/deberta-large/deberta-large/pytorch_model.bin\n","/kaggle/input/huggingface-deberta-variants/deberta-base/deberta-base/config.json\n","/kaggle/input/huggingface-deberta-variants/deberta-base/deberta-base/merges.txt\n","/kaggle/input/huggingface-deberta-variants/deberta-base/deberta-base/vocab.json\n","/kaggle/input/huggingface-deberta-variants/deberta-base/deberta-base/tokenizer_config.json\n","/kaggle/input/huggingface-deberta-variants/deberta-base/deberta-base/bpe_encoder.bin\n","/kaggle/input/huggingface-deberta-variants/deberta-base/deberta-base/pytorch_model.bin\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# Load libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T15:24:08.585397Z","iopub.status.busy":"2022-10-30T15:24:08.584487Z","iopub.status.idle":"2022-10-30T15:24:08.598001Z","shell.execute_reply":"2022-10-30T15:24:08.596956Z","shell.execute_reply.started":"2022-10-30T15:24:08.585343Z"},"trusted":true},"outputs":[],"source":["import os, gc\n","import numpy as np \n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","from tensorflow.keras import layers\n","\n","import transformers\n","from transformers import TFAutoModel, AutoTokenizer, AutoConfig\n","import sys \n","sys.path.append('../input/iterativestratification')\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n"]},{"cell_type":"markdown","metadata":{},"source":["# Import Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T15:24:11.526973Z","iopub.status.busy":"2022-10-30T15:24:11.526347Z","iopub.status.idle":"2022-10-30T15:24:11.640423Z","shell.execute_reply":"2022-10-30T15:24:11.639358Z","shell.execute_reply.started":"2022-10-30T15:24:11.526934Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(        text_id                                          full_text  cohesion  \\\n"," 0  0016926B079C  I think that students would benefit from learn...       3.5   \n"," 1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n"," 2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n"," 3  003885A45F42  The best time in life is when you become yours...       4.5   \n"," 4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n"," \n","    syntax  vocabulary  phraseology  grammar  conventions  \n"," 0     3.5         3.0          3.0      4.0          3.0  \n"," 1     2.5         3.0          2.0      2.0          2.5  \n"," 2     3.5         3.0          3.0      3.0          2.5  \n"," 3     4.5         4.5          4.5      4.0          5.0  \n"," 4     3.0         3.0          3.0      2.5          2.5  ,\n","         text_id                                          full_text\n"," 0  0000C359D63E  when a person has no experience on a job their...\n"," 1  000BAD50D026  Do you think students would benefit from being...\n"," 2  00367BB2546B  Thomas Jefferson once states that \"it is wonde...,\n","         text_id  cohesion  syntax  vocabulary  phraseology  grammar  \\\n"," 0  0000C359D63E       3.0     3.0         3.0          3.0      3.0   \n"," 1  000BAD50D026       3.0     3.0         3.0          3.0      3.0   \n"," 2  00367BB2546B       3.0     3.0         3.0          3.0      3.0   \n"," \n","    conventions  \n"," 0          3.0  \n"," 1          3.0  \n"," 2          3.0  )"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv('../input/feedback-prize-english-language-learning/train.csv')\n","test = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n","sample_submission = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')\n","\n","train.head(), test.head(), sample_submission.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Create Cross-Validation split"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T15:24:13.871843Z","iopub.status.busy":"2022-10-30T15:24:13.871457Z","iopub.status.idle":"2022-10-30T15:24:13.877107Z","shell.execute_reply":"2022-10-30T15:24:13.875598Z","shell.execute_reply.started":"2022-10-30T15:24:13.871812Z"},"trusted":true},"outputs":[],"source":["# create 5 folds for cross validation\n","N_FOLD = 5\n","TARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T15:24:20.341885Z","iopub.status.busy":"2022-10-30T15:24:20.341501Z","iopub.status.idle":"2022-10-30T15:24:20.497158Z","shell.execute_reply":"2022-10-30T15:24:20.495669Z","shell.execute_reply.started":"2022-10-30T15:24:20.341853Z"},"trusted":true},"outputs":[{"data":{"text/plain":["1    783\n","0    782\n","4    782\n","3    782\n","2    782\n","Name: fold, dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# apply kfold on dataset\n","folds = MultilabelStratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=42)\n","\n","for n, (train_index, val_index) in enumerate(folds.split(train, train[TARGET_COLS])):\n","    train.loc[val_index, 'fold'] = int(n)\n","\n","train['fold'] = train['fold'].astype(int)\n","train['fold'].value_counts()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T15:24:26.080651Z","iopub.status.busy":"2022-10-30T15:24:26.080062Z","iopub.status.idle":"2022-10-30T15:24:26.111166Z","shell.execute_reply":"2022-10-30T15:24:26.110075Z","shell.execute_reply.started":"2022-10-30T15:24:26.080605Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_id</th>\n","      <th>full_text</th>\n","      <th>cohesion</th>\n","      <th>syntax</th>\n","      <th>vocabulary</th>\n","      <th>phraseology</th>\n","      <th>grammar</th>\n","      <th>conventions</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0016926B079C</td>\n","      <td>I think that students would benefit from learn...</td>\n","      <td>3.5</td>\n","      <td>3.5</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0022683E9EA5</td>\n","      <td>When a problem is a change you have to let it ...</td>\n","      <td>2.5</td>\n","      <td>2.5</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.5</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00299B378633</td>\n","      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n","      <td>3.0</td>\n","      <td>3.5</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.5</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>003885A45F42</td>\n","      <td>The best time in life is when you become yours...</td>\n","      <td>4.5</td>\n","      <td>4.5</td>\n","      <td>4.5</td>\n","      <td>4.5</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0049B1DF5CCC</td>\n","      <td>Small act of kindness can impact in other peop...</td>\n","      <td>2.5</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2.5</td>\n","      <td>2.5</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        text_id                                          full_text  cohesion  \\\n","0  0016926B079C  I think that students would benefit from learn...       3.5   \n","1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n","2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n","3  003885A45F42  The best time in life is when you become yours...       4.5   \n","4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n","\n","   syntax  vocabulary  phraseology  grammar  conventions  fold  \n","0     3.5         3.0          3.0      4.0          3.0     1  \n","1     2.5         3.0          2.0      2.0          2.5     0  \n","2     3.5         3.0          3.0      3.0          2.5     4  \n","3     4.5         4.5          4.5      4.0          5.0     3  \n","4     3.0         3.0          3.0      2.5          2.5     1  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# check train dataset after adding fold column\n","train.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Model configuration"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T15:25:23.371171Z","iopub.status.busy":"2022-10-30T15:25:23.370795Z","iopub.status.idle":"2022-10-30T15:25:23.376540Z","shell.execute_reply":"2022-10-30T15:25:23.375485Z","shell.execute_reply.started":"2022-10-30T15:25:23.371139Z"},"trusted":true},"outputs":[],"source":["# standard config\n","MAX_LENGTH = 512\n","BATCH_SIZE = 8\n","\n","MODEL = \"../input/roberta-base/\""]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T15:25:58.760242Z","iopub.status.busy":"2022-10-30T15:25:58.759859Z","iopub.status.idle":"2022-10-30T15:26:08.217256Z","shell.execute_reply":"2022-10-30T15:26:08.216142Z","shell.execute_reply.started":"2022-10-30T15:25:58.760209Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-10-30 15:25:59.439790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-10-30 15:25:59.440866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-10-30 15:25:59.441580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-10-30 15:25:59.442491: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-10-30 15:25:59.442846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-10-30 15:25:59.443577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-10-30 15:25:59.444220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-10-30 15:26:01.722720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-10-30 15:26:01.723621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-10-30 15:26:01.724309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-10-30 15:26:01.724908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15043 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","Some layers from the model checkpoint at ../input/roberta-base/ were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/roberta-base/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]}],"source":["# load our models\n","\n","model = TFAutoModel.from_pretrained(MODEL)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","\n","CFG = transformers.AutoConfig.from_pretrained(MODEL)\n","CFG.hidden_dropout_prob = 0.\n","CFG.attention_probs_dropout_prob = 0."]},{"cell_type":"markdown","metadata":{},"source":["# Data Processing Functions"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T15:26:11.170318Z","iopub.status.busy":"2022-10-30T15:26:11.169126Z","iopub.status.idle":"2022-10-30T15:26:11.177760Z","shell.execute_reply":"2022-10-30T15:26:11.176361Z","shell.execute_reply.started":"2022-10-30T15:26:11.170254Z"},"trusted":true},"outputs":[],"source":["def CreateEmbeddings(texts, tokenizer=tokenizer):\n","    \n","    \"\"\"Generate embeddings and attention_masks from text \"\"\"\n","\n","    input_ids = []\n","    attention_mask = []\n","    \n","    for text in texts.to_list():\n","        token = tokenizer(text, \n","                          add_special_tokens=True,\n","                          max_length=MAX_LENGTH,\n","                          return_attention_mask=True, \n","                          return_tensors='np',\n","                          truncation=True,\n","                          padding='max_length')\n","        input_ids.append(token['input_ids'][0])\n","        attention_mask.append(token['attention_mask'][0])\n","    \n","    return np.array(input_ids), np.array(attention_mask) "]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T15:46:45.226479Z","iopub.status.busy":"2022-10-30T15:46:45.226083Z","iopub.status.idle":"2022-10-30T15:46:45.232005Z","shell.execute_reply":"2022-10-30T15:46:45.230525Z","shell.execute_reply.started":"2022-10-30T15:46:45.226445Z"},"trusted":true},"outputs":[],"source":["def GetDataset(dataframe):\n","    \n","    \"\"\"Create dataset object from dataframe\"\"\"\n","    \n","    inputs = CreateEmbeddings(dataframe['full_text'])\n","    targets = np.array(dataframe[TARGET_COLS])\n","    \n","    return inputs, targets"]},{"cell_type":"markdown","metadata":{},"source":["# MeanPooling Layer\n","\n","Instead of using '[CLS]' token, MeanPool layer averaging roberta's last hidden states along the sequence axis with masking out padding tokens."]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T15:46:51.971099Z","iopub.status.busy":"2022-10-30T15:46:51.970418Z","iopub.status.idle":"2022-10-30T15:46:51.979383Z","shell.execute_reply":"2022-10-30T15:46:51.977120Z","shell.execute_reply.started":"2022-10-30T15:46:51.971049Z"},"trusted":true},"outputs":[],"source":["class MeanPooling(tf.keras.layers.Layer):\n","    \n","    def call(self, inputs, mask=None):\n","        \n","        \"\"\"Generate sentence embeddings for each text by applying meanpooling \"\"\"\n","        \n","        total_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n","        embedding_sum = tf.reduce_sum(inputs * total_mask, axis=1)\n","        mask_sum = tf.reduce_sum(total_mask, axis=1)\n","        mask_sum = tf.math.maximum(mask_sum, tf.constant([1e-9]))\n","        \n","        return embedding_sum / mask_sum"]},{"cell_type":"markdown","metadata":{},"source":["# Build the Model"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T15:46:54.328359Z","iopub.status.busy":"2022-10-30T15:46:54.327961Z","iopub.status.idle":"2022-10-30T15:46:54.335981Z","shell.execute_reply":"2022-10-30T15:46:54.334930Z","shell.execute_reply.started":"2022-10-30T15:46:54.328321Z"},"trusted":true},"outputs":[],"source":["def MainModel():\n","\n","    \"\"\" Create the fully connected Neural Network model from the output embeddings\"\"\"\n","    \n","    input_ids = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name=\"input_ids\")\n","    attention_masks = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name=\"attention_masks\")\n","   \n","    model = transformers.TFAutoModel.from_pretrained(MODEL, config=CFG)\n","    model_output = model.roberta(input_ids, attention_mask=attention_masks)\n","    \n","    x = model_output.last_hidden_state\n","    x = MeanPooling()(x, mask=attention_masks)\n","    x = layers.Dense(6, activation='sigmoid')(x)\n","    \n","    output = layers.Rescaling(scale=4.0, offset=1.0)(x)\n","    model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output)\n","    \n","    return model"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T15:46:57.538194Z","iopub.status.busy":"2022-10-30T15:46:57.537823Z","iopub.status.idle":"2022-10-30T15:47:04.301358Z","shell.execute_reply":"2022-10-30T15:47:04.300348Z","shell.execute_reply.started":"2022-10-30T15:46:57.538161Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at ../input/roberta-base/ were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/roberta-base/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_ids (InputLayer)          [(None, 512)]        0                                            \n","__________________________________________________________________________________________________\n","attention_masks (InputLayer)    [(None, 512)]        0                                            \n","__________________________________________________________________________________________________\n","roberta (TFRobertaMainLayer)    TFBaseModelOutputWit 124645632   input_ids[0][0]                  \n","                                                                 attention_masks[0][0]            \n","__________________________________________________________________________________________________\n","mean_pooling (MeanPooling)      (None, 768)          0           roberta[0][0]                    \n","                                                                 attention_masks[0][0]            \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 6)            4614        mean_pooling[0][0]               \n","__________________________________________________________________________________________________\n","rescaling (Rescaling)           (None, 6)            0           dense[0][0]                      \n","==================================================================================================\n","Total params: 124,650,246\n","Trainable params: 124,650,246\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["# review model structure\n","tf.keras.backend.clear_session()\n","model = MainModel()\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["# 5 Folds training loop"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T16:00:30.818570Z","iopub.status.busy":"2022-10-30T16:00:30.817459Z","iopub.status.idle":"2022-10-30T18:25:11.926380Z","shell.execute_reply":"2022-10-30T18:25:11.925318Z","shell.execute_reply.started":"2022-10-30T16:00:30.818526Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","=== FOLD 0 ===\n","=== Data loaded ===\n"]},{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at ../input/roberta-base/ were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/roberta-base/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["=== Start training ===\n","Epoch 1/10\n","392/392 [==============================] - 246s 593ms/step - loss: 0.2652 - root_mean_squared_error: 0.5150 - val_loss: 0.2166 - val_root_mean_squared_error: 0.4654\n","\n","Epoch 00001: val_loss improved from inf to 0.21662, saving model to best_model_fold0.h5\n","Epoch 2/10\n","392/392 [==============================] - 229s 585ms/step - loss: 0.2039 - root_mean_squared_error: 0.4515 - val_loss: 0.2096 - val_root_mean_squared_error: 0.4578\n","\n","Epoch 00002: val_loss improved from 0.21662 to 0.20958, saving model to best_model_fold0.h5\n","Epoch 3/10\n","392/392 [==============================] - 229s 585ms/step - loss: 0.1856 - root_mean_squared_error: 0.4308 - val_loss: 0.2190 - val_root_mean_squared_error: 0.4679\n","\n","Epoch 00003: val_loss did not improve from 0.20958\n","Epoch 4/10\n","392/392 [==============================] - 229s 585ms/step - loss: 0.1731 - root_mean_squared_error: 0.4161 - val_loss: 0.2094 - val_root_mean_squared_error: 0.4576\n","\n","Epoch 00004: val_loss improved from 0.20958 to 0.20944, saving model to best_model_fold0.h5\n","Epoch 5/10\n","392/392 [==============================] - 229s 585ms/step - loss: 0.1658 - root_mean_squared_error: 0.4071 - val_loss: 0.2089 - val_root_mean_squared_error: 0.4571\n","\n","Epoch 00005: val_loss improved from 0.20944 to 0.20892, saving model to best_model_fold0.h5\n","Epoch 6/10\n","392/392 [==============================] - 229s 585ms/step - loss: 0.1619 - root_mean_squared_error: 0.4024 - val_loss: 0.2123 - val_root_mean_squared_error: 0.4607\n","\n","Epoch 00006: val_loss did not improve from 0.20892\n","Epoch 7/10\n","392/392 [==============================] - 229s 585ms/step - loss: 0.1596 - root_mean_squared_error: 0.3995 - val_loss: 0.2114 - val_root_mean_squared_error: 0.4598\n","\n","Epoch 00007: val_loss did not improve from 0.20892\n","Epoch 8/10\n","392/392 [==============================] - 229s 585ms/step - loss: 0.1582 - root_mean_squared_error: 0.3978 - val_loss: 0.2105 - val_root_mean_squared_error: 0.4588\n","\n","Epoch 00008: val_loss did not improve from 0.20892\n","Epoch 00008: early stopping\n","=== Training finished ===\n","\n","=== FOLD 1 ===\n","=== Data loaded ===\n"]},{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at ../input/roberta-base/ were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/roberta-base/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["=== Start training ===\n","Epoch 1/10\n","391/391 [==============================] - 230s 566ms/step - loss: 0.2594 - root_mean_squared_error: 0.5093 - val_loss: 0.2178 - val_root_mean_squared_error: 0.4667\n","\n","Epoch 00001: val_loss improved from inf to 0.21777, saving model to best_model_fold1.h5\n","Epoch 2/10\n","391/391 [==============================] - 219s 560ms/step - loss: 0.1981 - root_mean_squared_error: 0.4451 - val_loss: 0.2184 - val_root_mean_squared_error: 0.4673\n","\n","Epoch 00002: val_loss did not improve from 0.21777\n","Epoch 3/10\n","391/391 [==============================] - 219s 560ms/step - loss: 0.1769 - root_mean_squared_error: 0.4206 - val_loss: 0.2170 - val_root_mean_squared_error: 0.4658\n","\n","Epoch 00003: val_loss improved from 0.21777 to 0.21697, saving model to best_model_fold1.h5\n","Epoch 4/10\n","391/391 [==============================] - 219s 560ms/step - loss: 0.1633 - root_mean_squared_error: 0.4041 - val_loss: 0.2152 - val_root_mean_squared_error: 0.4638\n","\n","Epoch 00004: val_loss improved from 0.21697 to 0.21515, saving model to best_model_fold1.h5\n","Epoch 5/10\n","391/391 [==============================] - 219s 560ms/step - loss: 0.1560 - root_mean_squared_error: 0.3950 - val_loss: 0.2168 - val_root_mean_squared_error: 0.4656\n","\n","Epoch 00005: val_loss did not improve from 0.21515\n","Epoch 6/10\n","391/391 [==============================] - 219s 560ms/step - loss: 0.1514 - root_mean_squared_error: 0.3891 - val_loss: 0.2161 - val_root_mean_squared_error: 0.4649\n","\n","Epoch 00006: val_loss did not improve from 0.21515\n","Epoch 7/10\n","391/391 [==============================] - 219s 560ms/step - loss: 0.1487 - root_mean_squared_error: 0.3857 - val_loss: 0.2158 - val_root_mean_squared_error: 0.4645\n","\n","Epoch 00007: val_loss did not improve from 0.21515\n","Epoch 00007: early stopping\n","=== Training finished ===\n","\n","=== FOLD 2 ===\n","=== Data loaded ===\n"]},{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at ../input/roberta-base/ were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/roberta-base/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["=== Start training ===\n","Epoch 1/10\n","392/392 [==============================] - 245s 591ms/step - loss: 0.2456 - root_mean_squared_error: 0.4956 - val_loss: 0.2241 - val_root_mean_squared_error: 0.4734\n","\n","Epoch 00001: val_loss improved from inf to 0.22414, saving model to best_model_fold2.h5\n","Epoch 2/10\n","392/392 [==============================] - 229s 584ms/step - loss: 0.1967 - root_mean_squared_error: 0.4435 - val_loss: 0.2197 - val_root_mean_squared_error: 0.4687\n","\n","Epoch 00002: val_loss improved from 0.22414 to 0.21967, saving model to best_model_fold2.h5\n","Epoch 3/10\n","392/392 [==============================] - 229s 585ms/step - loss: 0.1809 - root_mean_squared_error: 0.4254 - val_loss: 0.2169 - val_root_mean_squared_error: 0.4657\n","\n","Epoch 00003: val_loss improved from 0.21967 to 0.21690, saving model to best_model_fold2.h5\n","Epoch 4/10\n","392/392 [==============================] - 229s 585ms/step - loss: 0.1696 - root_mean_squared_error: 0.4118 - val_loss: 0.2180 - val_root_mean_squared_error: 0.4669\n","\n","Epoch 00004: val_loss did not improve from 0.21690\n","Epoch 5/10\n","392/392 [==============================] - 229s 584ms/step - loss: 0.1631 - root_mean_squared_error: 0.4039 - val_loss: 0.2168 - val_root_mean_squared_error: 0.4657\n","\n","Epoch 00005: val_loss improved from 0.21690 to 0.21683, saving model to best_model_fold2.h5\n","Epoch 6/10\n","392/392 [==============================] - 229s 584ms/step - loss: 0.1570 - root_mean_squared_error: 0.3962 - val_loss: 0.2198 - val_root_mean_squared_error: 0.4689\n","\n","Epoch 00007: val_loss did not improve from 0.21683\n","Epoch 8/10\n","392/392 [==============================] - 229s 584ms/step - loss: 0.1557 - root_mean_squared_error: 0.3946 - val_loss: 0.2180 - val_root_mean_squared_error: 0.4669\n","\n","Epoch 00008: val_loss did not improve from 0.21683\n","Epoch 00008: early stopping\n","=== Training finished ===\n","\n","=== FOLD 3 ===\n","=== Data loaded ===\n"]},{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at ../input/roberta-base/ were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/roberta-base/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["=== Start training ===\n","Epoch 1/10\n","392/392 [==============================] - 245s 591ms/step - loss: 0.2604 - root_mean_squared_error: 0.5103 - val_loss: 0.2227 - val_root_mean_squared_error: 0.4719\n","\n","Epoch 00001: val_loss improved from inf to 0.22268, saving model to best_model_fold3.h5\n","Epoch 2/10\n","392/392 [==============================] - 229s 584ms/step - loss: 0.2017 - root_mean_squared_error: 0.4491 - val_loss: 0.2314 - val_root_mean_squared_error: 0.4810\n","\n","Epoch 00002: val_loss did not improve from 0.22268\n","Epoch 3/10\n","392/392 [==============================] - 229s 584ms/step - loss: 0.1841 - root_mean_squared_error: 0.4291 - val_loss: 0.2125 - val_root_mean_squared_error: 0.4610\n","\n","Epoch 00003: val_loss improved from 0.22268 to 0.21254, saving model to best_model_fold3.h5\n","Epoch 4/10\n","392/392 [==============================] - 229s 584ms/step - loss: 0.1728 - root_mean_squared_error: 0.4157 - val_loss: 0.2133 - val_root_mean_squared_error: 0.4618\n","\n","Epoch 00004: val_loss did not improve from 0.21254\n","Epoch 5/10\n","392/392 [==============================] - 229s 585ms/step - loss: 0.1657 - root_mean_squared_error: 0.4070 - val_loss: 0.2134 - val_root_mean_squared_error: 0.4619\n","\n","Epoch 00005: val_loss did not improve from 0.21254\n","Epoch 6/10\n","392/392 [==============================] - 229s 584ms/step - loss: 0.1619 - root_mean_squared_error: 0.4024 - val_loss: 0.2137 - val_root_mean_squared_error: 0.4623\n","\n","Epoch 00006: val_loss did not improve from 0.21254\n","Epoch 00006: early stopping\n","=== Training finished ===\n","\n","=== FOLD 4 ===\n","=== Data loaded ===\n"]},{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at ../input/roberta-base/ were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/roberta-base/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["=== Start training ===\n","Epoch 1/10\n","392/392 [==============================] - 245s 591ms/step - loss: 0.2627 - root_mean_squared_error: 0.5125 - val_loss: 0.2221 - val_root_mean_squared_error: 0.4712\n","\n","Epoch 00001: val_loss improved from inf to 0.22206, saving model to best_model_fold4.h5\n","Epoch 2/10\n","392/392 [==============================] - 229s 585ms/step - loss: 0.1990 - root_mean_squared_error: 0.4461 - val_loss: 0.2146 - val_root_mean_squared_error: 0.4632\n","\n","Epoch 00002: val_loss improved from 0.22206 to 0.21456, saving model to best_model_fold4.h5\n","Epoch 3/10\n","392/392 [==============================] - 229s 585ms/step - loss: 0.1786 - root_mean_squared_error: 0.4227 - val_loss: 0.2105 - val_root_mean_squared_error: 0.4588\n","\n","Epoch 00003: val_loss improved from 0.21456 to 0.21049, saving model to best_model_fold4.h5\n","Epoch 4/10\n","392/392 [==============================] - 229s 584ms/step - loss: 0.1664 - root_mean_squared_error: 0.4079 - val_loss: 0.2135 - val_root_mean_squared_error: 0.4620\n","\n","Epoch 00004: val_loss did not improve from 0.21049\n","Epoch 5/10\n","392/392 [==============================] - 229s 585ms/step - loss: 0.1577 - root_mean_squared_error: 0.3971 - val_loss: 0.2102 - val_root_mean_squared_error: 0.4585\n","\n","Epoch 00005: val_loss improved from 0.21049 to 0.21018, saving model to best_model_fold4.h5\n","Epoch 6/10\n","392/392 [==============================] - 229s 584ms/step - loss: 0.1528 - root_mean_squared_error: 0.3909 - val_loss: 0.2120 - val_root_mean_squared_error: 0.4605\n","\n","Epoch 00006: val_loss did not improve from 0.21018\n","Epoch 7/10\n","392/392 [==============================] - 229s 584ms/step - loss: 0.1502 - root_mean_squared_error: 0.3876 - val_loss: 0.2115 - val_root_mean_squared_error: 0.4599\n","\n","Epoch 00007: val_loss did not improve from 0.21018\n","Epoch 8/10\n","392/392 [==============================] - 229s 584ms/step - loss: 0.1487 - root_mean_squared_error: 0.3856 - val_loss: 0.2119 - val_root_mean_squared_error: 0.4603\n","\n","Epoch 00008: val_loss did not improve from 0.21018\n","Epoch 00008: early stopping\n","=== Training finished ===\n"]}],"source":["# create 5 training loop and store results of best folds using callbacks\n","\n","valid_rmses = []\n","\n","for fold in range(N_FOLD):\n","    print(f'\\n=== FOLD {fold} ===')\n","    \n","    # Generate dataset\n","    train_df = train[train['fold'] != fold].reset_index(drop=True)\n","    valid_df = train[train['fold'] == fold].reset_index(drop=True)\n","    train_dataset = GetDataset(train_df)\n","    valid_dataset = GetDataset(valid_df)\n","    \n","    print('=== Data loaded ===')\n","    \n","    #Create model\n","    tf.keras.backend.clear_session()\n","    \n","    model = MainModel()\n","    \n","    # Compile model with layer-wise learning rate decay\n","    LR_SCH_DECAY_STEPS = 2 * len(train_df) // BATCH_SIZE\n","    \n","    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","                                initial_learning_rate=1e-5, \n","                                decay_steps=LR_SCH_DECAY_STEPS, \n","                                decay_rate=0.3)\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n","\n","    model.compile(optimizer=optimizer,\n","                 loss='mean_squared_error',\n","                 metrics=[tf.keras.metrics.RootMeanSquaredError()],\n","                 )\n","\n","    \n","    #Training model\n","    print('=== Start training ===')\n","    callbacks = [\n","                tf.keras.callbacks.ModelCheckpoint(f\"best_model_fold{fold}.h5\",\n","                                                   monitor=\"val_loss\",\n","                                                   mode=\"min\",\n","                                                   save_best_only=True,\n","                                                   verbose=1,\n","                                                   save_weights_only=True,),\n","                tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n","                                                 min_delta=1e-5, \n","                                                 patience=3, \n","                                                 verbose=1,\n","                                                 mode='min',)\n","                ]\n","    \n","    history = model.fit(x=train_dataset[0],\n","                        y=train_dataset[1],\n","                        validation_data=valid_dataset, \n","                        epochs=10,\n","                        shuffle=True,\n","                        batch_size=BATCH_SIZE,\n","                        callbacks=callbacks\n","                       )\n","    \n","    valid_rmses.append(np.min(history.history['val_root_mean_squared_error']))\n","    \n","    print('=== Training finished ===')\n","    \n","    del train_dataset, valid_dataset, train_df, valid_df\n","    \n","    gc.collect()"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T18:28:04.735921Z","iopub.status.busy":"2022-10-30T18:28:04.735520Z","iopub.status.idle":"2022-10-30T18:28:04.742218Z","shell.execute_reply":"2022-10-30T18:28:04.740882Z","shell.execute_reply.started":"2022-10-30T18:28:04.735885Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["5 Folds validation RMSE:\n","[0.45707303285598755, 0.4638441503047943, 0.46565210819244385, 0.4610193371772766, 0.4584513008594513]\n","Local CV Average score: 0.4612079858779907\n"]}],"source":["# print average score by folds\n","print(f'{len(valid_rmses)} Folds validation RMSE:\\n{valid_rmses}')\n","print(f'Local CV Average score: {np.mean(valid_rmses)}')"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T18:32:18.171923Z","iopub.status.busy":"2022-10-30T18:32:18.170861Z","iopub.status.idle":"2022-10-30T18:32:18.183193Z","shell.execute_reply":"2022-10-30T18:32:18.182002Z","shell.execute_reply.started":"2022-10-30T18:32:18.171875Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'loss': [0.26265501976013184, 0.19903627038002014, 0.17864340543746948, 0.16635850071907043, 0.15772423148155212, 0.15279991924762726, 0.15021324157714844, 0.14871016144752502], 'root_mean_squared_error': [0.5124987363815308, 0.44613468647003174, 0.4226624667644501, 0.4078707695007324, 0.39714503288269043, 0.3908964693546295, 0.3875734210014343, 0.3856296241283417], 'val_loss': [0.22205579280853271, 0.21455681324005127, 0.21048974990844727, 0.21347959339618683, 0.21017764508724213, 0.21203933656215668, 0.21148182451725006, 0.21188902854919434], 'val_root_mean_squared_error': [0.47122782468795776, 0.4632028043270111, 0.458791583776474, 0.4620385468006134, 0.4584513008594513, 0.4604773223400116, 0.4598715305328369, 0.4603140950202942]}\n"]}],"source":["# review history\n","print(history.history)"]},{"cell_type":"markdown","metadata":{},"source":["### Accessing Loss progress"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T18:33:07.612524Z","iopub.status.busy":"2022-10-30T18:33:07.611992Z","iopub.status.idle":"2022-10-30T18:33:07.866205Z","shell.execute_reply":"2022-10-30T18:33:07.865206Z","shell.execute_reply.started":"2022-10-30T18:33:07.612486Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['loss', 'root_mean_squared_error', 'val_loss', 'val_root_mean_squared_error'])\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# put the progress of loss into chart\n","print(history.history.keys())\n","# summarize history for accuracy\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Create prediction"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T18:26:39.462301Z","iopub.status.busy":"2022-10-30T18:26:39.461275Z","iopub.status.idle":"2022-10-30T18:26:39.473116Z","shell.execute_reply":"2022-10-30T18:26:39.472086Z","shell.execute_reply.started":"2022-10-30T18:26:39.462249Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_id</th>\n","      <th>full_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000C359D63E</td>\n","      <td>when a person has no experience on a job their...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000BAD50D026</td>\n","      <td>Do you think students would benefit from being...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00367BB2546B</td>\n","      <td>Thomas Jefferson once states that \"it is wonde...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        text_id                                          full_text\n","0  0000C359D63E  when a person has no experience on a job their...\n","1  000BAD50D026  Do you think students would benefit from being...\n","2  00367BB2546B  Thomas Jefferson once states that \"it is wonde..."]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["test.head()"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T18:29:08.118377Z","iopub.status.busy":"2022-10-30T18:29:08.117959Z","iopub.status.idle":"2022-10-30T18:29:08.134604Z","shell.execute_reply":"2022-10-30T18:29:08.133676Z","shell.execute_reply.started":"2022-10-30T18:29:08.118341Z"},"trusted":true},"outputs":[],"source":["# create dataset for test \n","test_dataset = CreateEmbeddings(test['full_text'])"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T18:29:26.596516Z","iopub.status.busy":"2022-10-30T18:29:26.595899Z","iopub.status.idle":"2022-10-30T18:30:16.096921Z","shell.execute_reply":"2022-10-30T18:30:16.095854Z","shell.execute_reply.started":"2022-10-30T18:29:26.596477Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at ../input/roberta-base/ were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/roberta-base/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Fold 0 predictions\n"]},{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at ../input/roberta-base/ were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/roberta-base/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Fold 1 predictions\n"]},{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at ../input/roberta-base/ were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/roberta-base/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Fold 2 predictions\n"]},{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at ../input/roberta-base/ were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/roberta-base/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Fold 3 predictions\n"]},{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at ../input/roberta-base/ were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/roberta-base/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Fold 4 predictions\n"]}],"source":["# create predictions\n","predictions = []\n","\n","# Loading weights from best performance folds\n","for fold in range(5):\n","    tf.keras.backend.clear_session()\n","    model = MainModel()\n","    model.load_weights(f'best_model_fold{fold}.h5')\n","    print(f'\\nFold {fold} predictions')\n","    prediction = model.predict(test_dataset, batch_size=8)\n","    predictions.append(prediction)\n","    gc.collect()"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T18:30:32.157781Z","iopub.status.busy":"2022-10-30T18:30:32.157406Z","iopub.status.idle":"2022-10-30T18:30:32.179218Z","shell.execute_reply":"2022-10-30T18:30:32.178307Z","shell.execute_reply.started":"2022-10-30T18:30:32.157752Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_id</th>\n","      <th>cohesion</th>\n","      <th>syntax</th>\n","      <th>vocabulary</th>\n","      <th>phraseology</th>\n","      <th>grammar</th>\n","      <th>conventions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000C359D63E</td>\n","      <td>2.863481</td>\n","      <td>2.774186</td>\n","      <td>3.036548</td>\n","      <td>2.929232</td>\n","      <td>2.681797</td>\n","      <td>2.575963</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000BAD50D026</td>\n","      <td>2.764522</td>\n","      <td>2.541825</td>\n","      <td>2.760355</td>\n","      <td>2.452696</td>\n","      <td>2.234032</td>\n","      <td>2.722997</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00367BB2546B</td>\n","      <td>3.560313</td>\n","      <td>3.387542</td>\n","      <td>3.597499</td>\n","      <td>3.554564</td>\n","      <td>3.367234</td>\n","      <td>3.327986</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n","0  0000C359D63E  2.863481  2.774186    3.036548     2.929232  2.681797   \n","1  000BAD50D026  2.764522  2.541825    2.760355     2.452696  2.234032   \n","2  00367BB2546B  3.560313  3.387542    3.597499     3.554564  3.367234   \n","\n","   conventions  \n","0     2.575963  \n","1     2.722997  \n","2     3.327986  "]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["# submit the predictions to Kaggle\n","\n","predictions = np.mean(predictions, axis=0)\n","\n","submission = pd.concat([test[['text_id']], pd.DataFrame(predictions, columns=TARGET_COLS)], axis=1)\n","\n","submission.to_csv('submission.csv', index=False)\n","\n","submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-10-30T18:30:51.899190Z","iopub.status.busy":"2022-10-30T18:30:51.898715Z","iopub.status.idle":"2022-10-30T18:30:51.908947Z","shell.execute_reply":"2022-10-30T18:30:51.907766Z","shell.execute_reply.started":"2022-10-30T18:30:51.899149Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<keras.callbacks.History at 0x7ff0225fd450>"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"},"vscode":{"interpreter":{"hash":"3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"}}},"nbformat":4,"nbformat_minor":4}
